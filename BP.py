import numpy as np
import json
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Establecer una semilla para garantizar que los resultados sean reproducibles
np.random.seed(0)

# Definici√≥n de funciones de activaci√≥n utilizadas en la red neuronal
def sigmoid(x):
    """Funci√≥n sigmoide: transforma valores en un rango de 0 a 1, √∫til para probabilidades"""
    return 1 / (1 + np.exp(-x))

def relu(x):
    """Funci√≥n ReLU: devuelve 0 para valores negativos y el valor original si es positivo"""
    return np.maximum(0, x)

def tanh(x):
    """Funci√≥n tangente hiperb√≥lica: transforma valores en un rango de -1 a 1"""
    return np.tanh(x)

class RedDeteccionFallas:
    def __init__(self, datos_entrada, respuestas_reales, neuronas_ocultas=4, activacion_oculta='sigmoid', activacion_salida='sigmoid'):
        """
        Inicializa la red neuronal con los datos de entrenamiento y par√°metros configurables.
        - datos_entrada: Matriz de datos de entrada (caracter√≠sticas normalizadas)
        - respuestas_reales: Vector de etiquetas reales (fallo=1, no fallo=0)
        - neuronas_ocultas: Cantidad de neuronas en la capa oculta
        - activacion_oculta: Tipo de funci√≥n de activaci√≥n para la capa oculta
        - activacion_salida: Tipo de funci√≥n de activaci√≥n para la capa de salida
        """
        # Imprimir las funciones de activaci√≥n seleccionadas para seguimiento
        print("FUNCI√ìN DE ACTIVACI√ìN OCULTA: " + activacion_oculta)
        print("FUNCI√ìN DE ACTIVACI√ìN SALIDA: " + activacion_salida)
        
        # Almacenar datos de entrenamiento y etiquetas
        self.datos_entrenamiento = datos_entrada  # Datos de entrada para entrenamiento
        self.respuestas_reales = respuestas_reales  # Etiquetas reales asociadas
        self.neuronas_ocultas = neuronas_ocultas  # N√∫mero de neuronas en la capa oculta
        self.caracteristicas_entrada = datos_entrada.shape[1]  # N√∫mero de caracter√≠sticas (3: temperatura, presi√≥n, corriente)
        self.activacion_oculta = activacion_oculta  # Funci√≥n de activaci√≥n de la capa oculta
        self.activacion_salida = activacion_salida  # Funci√≥n de activaci√≥n de la capa de salida
        
        # Inicializar pesos y sesgos con valores aleatorios peque√±os para evitar saturaci√≥n inicial
        self.pesos_entrada_oculta = np.random.randn(neuronas_ocultas, self.caracteristicas_entrada) * 0.1  # Pesos de entrada a oculta
        self.sesgos_oculta = np.zeros(neuronas_ocultas)  # Sesgos de la capa oculta (inician en 0)
        self.pesos_oculta_salida = np.random.randn(neuronas_ocultas) * 0.1  # Pesos de oculta a salida
        self.sesgo_salida = 0.0  # Sesgo de la capa de salida (inicia en 0)
        
        # Listas para almacenar el historial de m√©tricas durante el entrenamiento
        self.historial_error = []  # Registro del error acumulado por √©poca
        self.historial_accuracy = []  # Registro de la precisi√≥n por √©poca

    def _activacion(self, x, tipo):
        """Aplica la funci√≥n de activaci√≥n especificada al valor de entrada x"""
        if tipo == 'sigmoid':
            return sigmoid(x)  # Aplica sigmoide para valores entre 0 y 1
        elif tipo == 'relu':
            return relu(x)  # Aplica ReLU para valores no negativos
        elif tipo == 'tanh':
            return tanh(x)  # Aplica tanh para valores entre -1 y 1
        else:
            raise ValueError("Funci√≥n de activaci√≥n no soportada")

    def _derivada_activacion(self, x, tipo):
        """Calcula la derivada de la funci√≥n de activaci√≥n para usar en retropropagaci√≥n"""
        if tipo == 'sigmoid':
            return x * (1 - x)  # Derivada de sigmoide basada en la salida
        elif tipo == 'relu':
            return (x > 0).astype(float)  # Derivada de ReLU: 1 si x > 0, 0 si no
        elif tipo == 'tanh':
            return 1 - x**2  # Derivada de tanh basada en la salida
        else:
            raise ValueError("Derivada no implementada para esta activaci√≥n")

    def _interpretar_salida(self, valor_salida, tipo):
        """
        Interpreta la salida cruda de la red para convertirla en una predicci√≥n binaria (0 o 1).
        - valor_salida: Valor de salida de la capa final
        - tipo: Tipo de funci√≥n de activaci√≥n usada en la salida
        """
        if tipo == 'sigmoid':
            return 1 if valor_salida >= 0.5 else 0  # Umbral de 0.5 para clasificar como fallo/no fallo
        elif tipo == 'relu':
            return 1 if valor_salida > 0.5 else 0   # Umbral arbitrario para ReLU
        elif tipo == 'tanh':
            return 1 if valor_salida > 0 else 0     # Umbral en 0 para el rango -1 a 1 de tanh
        else:
            raise ValueError("Interpretaci√≥n no definida para esta activaci√≥n")

    def entrenar(self, ritmo_aprendizaje=0.01, vueltas_entrenamiento=1000, verbose=True):
        """
        Entrena la red neuronal utilizando el algoritmo de retropropagaci√≥n.
        - ritmo_aprendizaje: Tasa que controla el tama√±o de las actualizaciones de pesos
        - vueltas_entrenamiento: N√∫mero de iteraciones completas sobre el conjunto de entrenamiento
        - verbose: Si es True, muestra m√©tricas cada 10 √©pocas
        """
        for epoca in range(vueltas_entrenamiento):
            error_acumulado = 0.0  # Error total acumulado en esta √©poca
            predicciones_correctas = 0  # Contador de predicciones correctas
            
            # Iterar sobre cada muestra del conjunto de entrenamiento
            for i in range(self.datos_entrenamiento.shape[0]):
                # --- Propagaci√≥n hacia adelante ---
                entrada = self.datos_entrenamiento[i]  # Extraer la muestra de entrada actual
                
                # Calcular la salida de la capa oculta
                combinacion_lineal_oculta = np.dot(self.pesos_entrada_oculta, entrada) + self.sesgos_oculta  # Combinaci√≥n lineal
                salida_oculta = self._activacion(combinacion_lineal_oculta, self.activacion_oculta)  # Aplicar funci√≥n de activaci√≥n
                
                # Calcular la salida de la capa de salida
                combinacion_lineal_salida = np.dot(salida_oculta, self.pesos_oculta_salida) + self.sesgo_salida  # Combinaci√≥n lineal
                valor_salida = self._activacion(combinacion_lineal_salida, self.activacion_salida)  # Aplicar funci√≥n de activaci√≥n
                
                # --- Calcular error y precisi√≥n ---
                error = (valor_salida - self.respuestas_reales[i])**2  # Error cuadr√°tico entre predicci√≥n y etiqueta real
                error_acumulado += error  # Acumular error para la √©poca
                etiqueta_predicha = self._interpretar_salida(valor_salida, self.activacion_salida)  # Convertir a predicci√≥n binaria
                if etiqueta_predicha == self.respuestas_reales[i]:
                    predicciones_correctas += 1  # Incrementar contador si la predicci√≥n es correcta
                
                # --- Retropropagaci√≥n ---
                # Calcular el gradiente del error en la capa de salida
                gradiente_error_salida = (valor_salida - self.respuestas_reales[i]) * self._derivada_activacion(valor_salida, self.activacion_salida)
                
                # Calcular el gradiente del error en la capa oculta
                gradiente_error_oculta = gradiente_error_salida * self.pesos_oculta_salida * self._derivada_activacion(salida_oculta, self.activacion_oculta)
                
                # Actualizar pesos y sesgos usando el gradiente descendente
                self.pesos_oculta_salida -= ritmo_aprendizaje * gradiente_error_salida * salida_oculta  # Ajustar pesos oculta-salida
                self.sesgo_salida -= ritmo_aprendizaje * gradiente_error_salida  # Ajustar sesgo de salida
                
                # Actualizar pesos y sesgos de la capa oculta para cada neurona
                for neurona in range(self.neuronas_ocultas):
                    self.pesos_entrada_oculta[neurona] -= ritmo_aprendizaje * gradiente_error_oculta[neurona] * entrada
                    self.sesgos_oculta[neurona] -= ritmo_aprendizaje * gradiente_error_oculta[neurona]
            
            # --- Registrar m√©tricas de la √©poca ---
            self.historial_error.append(error_acumulado)  # Guardar error acumulado
            accuracy = predicciones_correctas / self.datos_entrenamiento.shape[0]  # Calcular precisi√≥n como proporci√≥n de aciertos
            self.historial_accuracy.append(accuracy)  # Guardar precisi√≥n
            
            # Mostrar progreso si verbose est√° activado
            if verbose and epoca % 10 == 0:
                print(f"√âpoca {epoca}: Error = {error_acumulado:.4f}, Accuracy = {accuracy:.2%}")

    def predecir(self, entrada):
        """
        Realiza una predicci√≥n para una muestra de entrada espec√≠fica.
        - entrada: Vector de caracter√≠sticas de una m√°quina
        """
        # Propagaci√≥n hacia adelante para la predicci√≥n
        combinacion_lineal_oculta = np.dot(self.pesos_entrada_oculta, entrada) + self.sesgos_oculta
        salida_oculta = self._activacion(combinacion_lineal_oculta, self.activacion_oculta)
        combinacion_lineal_salida = np.dot(salida_oculta, self.pesos_oculta_salida) + self.sesgo_salida
        valor_salida = self._activacion(combinacion_lineal_salida, self.activacion_salida)
        return self._interpretar_salida(valor_salida, self.activacion_salida)  # Devolver predicci√≥n binaria

    def evaluar(self, caracteristicas, etiquetas):
        """
        Eval√∫a el rendimiento de la red en un conjunto de prueba.
        - caracteristicas: Matriz de datos de prueba
        - etiquetas: Vector de etiquetas reales de prueba
        Retorna un diccionario con m√©tricas de evaluaci√≥n.
        """
        predicciones = np.array([self.predecir(x) for x in caracteristicas])  # Generar predicciones para todas las muestras
        etiquetas = np.array(etiquetas)  # Asegurar que las etiquetas sean un array NumPy
        
        # Calcular precisi√≥n global
        accuracy = np.mean(predicciones == etiquetas)
        
        # Calcular elementos de la matriz de confusi√≥n
        verdaderos_negativos = np.sum((etiquetas == 0) & (predicciones == 0))  # No fallo bien predicho
        falsos_positivos = np.sum((etiquetas == 0) & (predicciones == 1))  # No fallo predicho como fallo
        falsos_negativos = np.sum((etiquetas == 1) & (predicciones == 0))  # Fallo predicho como no fallo
        verdaderos_positivos = np.sum((etiquetas == 1) & (predicciones == 1))  # Fallo bien predicho
        
        # Calcular m√©tricas adicionales
        precision = verdaderos_positivos / (verdaderos_positivos + falsos_positivos) if (verdaderos_positivos + falsos_positivos) > 0 else 0
        recall = verdaderos_positivos / (verdaderos_positivos + falsos_negativos) if (verdaderos_positivos + falsos_negativos) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'accuracy': accuracy,  # Precisi√≥n global
            'precision': precision,  # Proporci√≥n de positivos correctos
            'recall': recall,  # Proporci√≥n de fallos detectados
            'f1': f1,  # Media arm√≥nica de precisi√≥n y recall
            'matriz_confusion': (verdaderos_negativos, falsos_positivos, falsos_negativos, verdaderos_positivos)  # Matriz de confusi√≥n
        }

# --- Procesamiento de datos ---
# Cargar los datos desde el archivo JSON
with open("maquinas.json", "r") as f:
    datos = json.load(f)

# Extraer caracter√≠sticas y etiquetas del archivo JSON
X = np.array([[m['temperatura'], m['presion'], m['corriente']] for m in datos])  # Matriz de caracter√≠sticas
y = np.array([m['fallo'] for m in datos])  # Vector de etiquetas (0 o 1)

# Normalizar datos al rango [0, 1] usando min-max
valores_minimos = X.min(axis=0)  # Valores m√≠nimos por caracter√≠stica
valores_maximos = X.max(axis=0)  # Valores m√°ximos por caracter√≠stica
X_normalizado = (X - valores_minimos) / (valores_maximos - valores_minimos)  # Normalizaci√≥n

# Dividir datos en entrenamiento (60%) y validaci√≥n (40%)
datos_entrenamiento, datos_validacion, respuestas_entrenamiento, respuestas_validacion = train_test_split(
    X_normalizado, y, test_size=0.4, random_state=0)

# --- Entrenamiento y evaluaci√≥n ---
activaciones = ['sigmoid', 'relu', 'tanh']  # Lista de funciones de activaci√≥n a probar
for activacion in activaciones:
    print(f"\n=== Evaluando con activaci√≥n de salida: {activacion} ===")
    
    # Crear y entrenar la red con la activaci√≥n seleccionada
    red = RedDeteccionFallas(datos_entrenamiento, respuestas_entrenamiento, neuronas_ocultas=6, 
                             activacion_oculta=activacion, activacion_salida=activacion)
    red.entrenar(ritmo_aprendizaje=0.01, vueltas_entrenamiento=300)  # Usar tasa baja como ejemplo

    # Evaluar en conjuntos de entrenamiento y validaci√≥n
    resultados_entrenamiento = red.evaluar(datos_entrenamiento, respuestas_entrenamiento)
    resultados_validacion = red.evaluar(datos_validacion, respuestas_validacion)

    # Imprimir resultados de entrenamiento
    print("\nResultados Entrenamiento:")
    print(f"Accuracy: {resultados_entrenamiento['accuracy']:.2%}")
    print(f"Precision: {resultados_entrenamiento['precision']:.2%}")
    print(f"Recall: {resultados_entrenamiento['recall']:.2%}")
    print(f"f1: {resultados_entrenamiento['f1']:.4f}")
    print(f"Matriz de Confusi√≥n (TN, FP, FN, TP): {resultados_entrenamiento['matriz_confusion']}")

    # Imprimir resultados de validaci√≥n
    print("\nResultados Validaci√≥n:")
    print(f"Accuracy: {resultados_validacion['accuracy']:.2%}")
    print(f"Precision: {resultados_validacion['precision']:.2%}")
    print(f"Recall: {resultados_validacion['recall']:.2%}")
    print(f"Matriz de Confusi√≥n (TN, FP, FN, TP): {resultados_validacion['matriz_confusion']}")

    # --- Visualizaci√≥n de m√©tricas ---
    plt.figure(figsize=(12, 5))
    
    # Gr√°fico del error
    plt.subplot(1, 2, 1)
    plt.plot(red.historial_error)
    plt.title(f'Curva de Aprendizaje ({activacion} en salida)')
    plt.xlabel('√âpoca')
    plt.ylabel('Error')
    
    # Gr√°fico de la precisi√≥n
    plt.subplot(1, 2, 2)
    plt.plot(red.historial_accuracy)
    plt.title(f'Precisi√≥n durante Entrenamiento ({activacion} en salida)')
    plt.xlabel('√âpoca')
    plt.ylabel('Accuracy')
    plt.tight_layout()
    plt.show()

    # --- Ejemplo de predicci√≥n con una nueva muestra ---
    nueva_maquina = np.array([85, 12, 23])  # Nueva m√°quina con valores de sensores
    nueva_maquina_normalizada = (nueva_maquina - valores_minimos) / (valores_maximos - valores_minimos)  # Normalizar
    prediccion = red.predecir(nueva_maquina_normalizada)
    print(f"Predicci√≥n para {nueva_maquina}: {'Falla üî¥' if prediccion == 1 else 'Normal ‚úÖ'}")